# COSC3500 Milestone 2 â€” K-Means Parallel Implementation

**Scope:** Parallel implementation of K-Means clustering using CUDA (primary) and OpenMP (comparison)
**Focus:** GPU acceleration of assign kernel (90% of runtime), scaling analysis, technology comparison
**Platform:** UQ HPC clusters with A100 GPUs (rangpur nodes)

## Implementation Status

### Completed âœ…
- M2 directory structure and organization
- Comprehensive Makefile for CUDA/OpenMP builds
- CUDA kernel implementations (basic, shared memory, transposed)
- Header files with common interface
- SLURM job submission scripts
- Benchmarking automation scripts
- Initial documentation framework

### In Progress ðŸ”„
- Testing and validation of CUDA kernels
- OpenMP implementation
- Performance profiling and optimization

### Planned ðŸ“‹
- Advanced CUDA optimizations (streams, mixed precision)
- Scaling studies (strong and weak)
- Cross-platform performance comparison
- Video presentation preparation

## Directory Organization

```
m2/                    # Main implementation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cuda/         # CUDA kernels and implementation
â”‚   â”œâ”€â”€ openmp/       # OpenMP parallel version
â”‚   â””â”€â”€ baseline/     # Serial reference (E2 from M1)
â”œâ”€â”€ include/          # Headers and interfaces
â””â”€â”€ Makefile         # Build system

m2-docs/              # Documentation
â”œâ”€â”€ design/          # Architecture and strategy
â”œâ”€â”€ experiments/     # Experiment logs and results
â””â”€â”€ profiling/       # Performance analysis

m2-bench/            # Benchmark results
â”œâ”€â”€ cuda/           # CUDA performance data
â”œâ”€â”€ openmp/         # OpenMP performance data
â””â”€â”€ comparison/     # Cross-platform analysis

m2-scripts/          # Automation
â”œâ”€â”€ slurm/          # HPC job scripts
â”œâ”€â”€ benchmark/      # Benchmarking scripts
â””â”€â”€ analysis/       # Data analysis tools
```

## Key Design Decisions

1. **CUDA Primary, OpenMP Secondary**: Focus depth on GPU implementation
2. **Preserve M1 Optimizations**: Build upon E2 (12% improvement)
3. **Memory-First Optimization**: Address bandwidth bottleneck
4. **Atomic Operations for Simplicity**: Initial update kernel approach
5. **Comprehensive Benchmarking**: Multiple problem sizes and configurations

## Performance Targets

### Baseline (Serial E2 from M1)
- Canonical (N=200K, D=16, K=8): 18.604ms
- Stress (N=100K, D=64, K=64): 292.040ms

### Minimum Goals
- 10x speedup with CUDA
- Correct convergence (matches serial)
- Handle all test configurations

### Target Goals
- 50x+ speedup on large problems
- Effective OpenMP parallelization (15-20x)
- Demonstrated scaling efficiency

## Technical Approach

### CUDA Strategy
1. **Basic Kernel**: One thread per point, global memory
2. **Shared Memory**: Cache centroids in shared memory (48KB limit)
3. **Transposed Layout**: Better coalescing for centroid access
4. **Advanced**: Warp primitives, texture memory, streams

### OpenMP Strategy
1. **Parallel For**: Simple parallelization over points
2. **NUMA Awareness**: Optimize for multi-socket systems
3. **Reduction Optimization**: Efficient centroid updates
4. **Thread Scaling**: 1-32 threads analysis

## Test Configurations

| Name | N | D | K | Purpose |
|------|---|---|---|---------|
| Tiny | 1K | 8 | 4 | Validation |
| Small | 10K | 16 | 8 | Quick test |
| Canonical | 200K | 16 | 8 | M1 baseline |
| Stress | 100K | 64 | 64 | Cache pressure |
| Medium | 500K | 32 | 32 | Scaling test |
| Large | 1M | 64 | 64 | GPU advantage |
| XLarge | 5M | 128 | 128 | Stress test |

## Development Timeline

### Week 1 (Days 1-5)
- âœ… Setup infrastructure
- âœ… Basic CUDA implementation
- âœ… Benchmarking framework
- ðŸ”„ Validation and correctness
- ðŸ“‹ Initial performance profiling

### Week 2 (Days 6-10)
- ðŸ“‹ Memory optimizations
- ðŸ“‹ OpenMP implementation
- ðŸ“‹ Advanced CUDA features
- ðŸ“‹ Scaling studies
- ðŸ“‹ Performance comparison

### Week 3 (Days 11-14)
- ðŸ“‹ Final optimizations
- ðŸ“‹ Documentation completion
- ðŸ“‹ Presentation preparation
- ðŸ“‹ Video recording
- ðŸ“‹ Submission preparation

## Build and Run

### Building
```bash
cd m2
make all          # Build all implementations
make cuda         # CUDA only
make openmp       # OpenMP only
make debug        # Debug builds
```

### Running
```bash
# CUDA
./kmeans_cuda -N 100000 -D 64 -K 32 -I 10 -S 42

# OpenMP
OMP_NUM_THREADS=16 ./kmeans_openmp -N 100000 -D 64 -K 32 -I 10 -S 42

# Full benchmark
../m2-scripts/benchmark/run_full_benchmark.sh
```

### SLURM Submission
```bash
sbatch m2-scripts/slurm/cuda_baseline.slurm
sbatch m2-scripts/slurm/openmp_baseline.slurm
sbatch m2-scripts/slurm/comparison_cpu_gpu.slurm
```

## Success Metrics

- **Correctness**: Inertia matches serial (Â±1e-5)
- **Performance**: >10x speedup minimum, >50x target
- **Scalability**: Linear weak scaling, >0.7 strong scaling efficiency
- **Documentation**: Complete experiment logs, profiling data
- **Presentation**: Clear 10-minute video, prepared for interview

## Key Files

- `m2/src/cuda/kmeans_cuda.cu`: Main CUDA implementation
- `m2/include/kmeans_cuda.cuh`: CUDA kernel declarations
- `m2/include/kmeans_common.hpp`: Shared interfaces
- `m2-docs/design/parallel_design.md`: Parallelization strategy
- `m2-scripts/benchmark/run_full_benchmark.sh`: Automated testing

## Notes

- Focus on assign kernel (90% of runtime)
- Memory bandwidth is likely bottleneck
- Shared memory limited to 48KB (A100)
- Atomic operations acceptable for update kernel
- Profile continuously with NSight Compute

---

*Project Start: 2025-10-25*
*Target Submission: Week of 2025-11-08*